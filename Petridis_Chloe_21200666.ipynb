{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       food                                        description\n",
      "0          Pain aux raisins  Pain aux raisins (French pronunciation: [pɛ̃ o...\n",
      "1                 Teurgoule  Teurgoule is a rice pudding that is a speciali...\n",
      "2                Pan bagnat  The pan bagnat (pronounced [pɑ̃ baˈɲa]) (pan b...\n",
      "3          Bichon au citron  The bichon au citron is a French pastry. It is...\n",
      "4        Poire belle Hélène  Poire belle Hélène ([pwaʁ bɛl elɛn]; German: B...\n",
      "..                      ...                                                ...\n",
      "182         Countess (cake)  The countess (kontès in Guianan Creole) is a s...\n",
      "183                Crespéou  A crespéou (French: [kʁɛs.pe.u]) is a savory P...\n",
      "184  Poire à la Beaujolaise  The Poire à la Beaujolaise or pear in wine is ...\n",
      "185         Akpan (dessert)  Akpan, also known as Akassa, is a fermented ma...\n",
      "186                 Pachade  A pachade, also known as a farinette, is a thi...\n",
      "\n",
      "[187 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5k/8l8nbdls3h5frfryvctk8r2w0000gn/T/ipykernel_1700/253569086.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  df_food_dataset=pd.read_csv('/Users/chloe/Documents/toto/L3S2/food_dataset.tsv', sep='\\t', error_bad_lines=False)\n"
     ]
    }
   ],
   "source": [
    "#Exerice 1.1\n",
    "\n",
    "import pandas as pd\n",
    "df_food_dataset=pd.read_csv('/Users/chloe/Documents/toto/L3S2/food_dataset.tsv', sep='\\t', error_bad_lines=False)\n",
    "print(df_food_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   000   10  100  110   12  1200  12th  1336  1344   14  ...  ˌkæfeɪ  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "5  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "6  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "7  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "8  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "9  0.0  0.0  0.0  0.0  0.0   0.0   0.0   0.0   0.0  0.0  ...     0.0   \n",
      "\n",
      "   ˌkæsəˈleɪ  ˌkæsʊˈleɪ  ˌkɒk  ˌmædlˈeɪn  ˌpɒtoʊˈfɜːr  ˌrætəˈtuːi  \\\n",
      "0        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "1        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "2        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "3        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "4        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "5        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "6        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "7        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "8        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "9        0.0        0.0   0.0        0.0          0.0         0.0   \n",
      "\n",
      "   ˌvɪʃiˈswɑːz  сир  трапист  \n",
      "0          0.0  0.0      0.0  \n",
      "1          0.0  0.0      0.0  \n",
      "2          0.0  0.0      0.0  \n",
      "3          0.0  0.0      0.0  \n",
      "4          0.0  0.0      0.0  \n",
      "5          0.0  0.0      0.0  \n",
      "6          0.0  0.0      0.0  \n",
      "7          0.0  0.0      0.0  \n",
      "8          0.0  0.0      0.0  \n",
      "9          0.0  0.0      0.0  \n",
      "\n",
      "[10 rows x 3768 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/chloe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Exercice 1.2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Je crée le transformateur TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('french'))\n",
    "\n",
    "# J'pplique le transformateur aux descriptions\n",
    "tfidf_matrix = vectorizer.fit_transform(df_food_dataset['description'])\n",
    "\n",
    "# Je convertis la matrice tf-idf en DataFrame et j'imprime des 10 premières lignes\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(tfidf_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice 1.3\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def calculate_similarity(food1, food2):\n",
    "    # J'obtiens les indices des deux aliments\n",
    "    index1 = df_food_dataset[df_food_dataset['food'] == food1].index[0]\n",
    "    index2 = df_food_dataset[df_food_dataset['food'] == food2].index[0]\n",
    "\n",
    "    # Je calcule et renvoie la distance cosinus\n",
    "    return 1 - cosine(tfidf_matrix[index1].toarray()[0], tfidf_matrix[index2].toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La similarité entre 'Pain aux raisins' et 'Tuile' est : 0.13485226422146168\n"
     ]
    }
   ],
   "source": [
    "#Exercice 1.4\n",
    "\n",
    "similarity_score = calculate_similarity('Pain aux raisins', 'Tuile')\n",
    "print(f\"La similarité entre 'Pain aux raisins' et 'Tuile' est : {similarity_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiant WordNet : star.n.04\n",
      "Définition : an actor who plays a principal role\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/chloe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/chloe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Exercice 2.1\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Je définis le texte et le mot à désambiguïser\n",
    "text = \"The astronomer loves the star of the movie who plays the lead role\"\n",
    "word = \"star\"\n",
    "\n",
    "# Je tokenise le texte\n",
    "tokenized_text = nltk.word_tokenize(text)\n",
    "\n",
    "# J'utilise l'algorithme Lesk pour désambiguïser le mot\n",
    "synset = lesk(tokenized_text, word)\n",
    "\n",
    "# J'affiche l'identifiant du synset et sa définition\n",
    "print(f\"Identifiant WordNet : {synset.name()}\")\n",
    "print(f\"Définition : {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercice 2.2\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Je crée une fonction pour retirer les stopwords d'une liste de mots\n",
    "def remove_stopwords(word_list):\n",
    "    return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "def enhanced_lesk(context_sentence, ambiguous_word, pos=None, synsets=None):\n",
    "    # Tokenization de la phrase et suppression des stopwords\n",
    "    context = set(remove_stopwords(word_tokenize(context_sentence)))\n",
    "    \n",
    "    if synsets is None:\n",
    "        synsets = wn.synsets(ambiguous_word)\n",
    "\n",
    "    if pos:\n",
    "        synsets = [ss for ss in synsets if str(ss.pos()) == pos]\n",
    "\n",
    "    if not synsets:\n",
    "        return None\n",
    "\n",
    "    max_overlap = 0\n",
    "    best_sense = None\n",
    "\n",
    "    for ss in synsets:\n",
    "        # J'ajoute la définition du sens\n",
    "        sense_context = set(remove_stopwords(word_tokenize(ss.definition())))\n",
    "        \n",
    "        # J'ajoute les phrases d'exemple\n",
    "        for example in ss.examples():\n",
    "            sense_context = sense_context.union(set(remove_stopwords(word_tokenize(example))))\n",
    "            \n",
    "        # J'ajoute les définitions des hyperonymes directs\n",
    "        for hyper in ss.hypernyms():\n",
    "            sense_context = sense_context.union(set(remove_stopwords(word_tokenize(hyper.definition()))))\n",
    "\n",
    "        # Je calcule l'intersection entre le contexte du sens et le contexte de la phrase\n",
    "        overlap = len(context.intersection(sense_context))\n",
    "\n",
    "        # Si l'intersection est plus grande que l'intersection maximale précédente, je mets à jour le meilleur sens et la valeur maximale d'intersection\n",
    "        if overlap > max_overlap:\n",
    "            max_overlap = overlap\n",
    "            best_sense = ss\n",
    "\n",
    "            return best_sense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiant WordNet : star.n.01\n",
      "Définition : (astronomy) a celestial body of hot gases that radiates energy derived from thermonuclear reactions in the interior\n"
     ]
    }
   ],
   "source": [
    "context_sentence = \"The astronomer loves the star that twinckles in the sky so bright\"\n",
    "ambiguous_word = \"star\"\n",
    "synset = enhanced_lesk(context_sentence, ambiguous_word)\n",
    "\n",
    "print(f\"Identifiant WordNet : {synset.name()}\")\n",
    "print(f\"Définition : {synset.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book: http://dbpedia.org/resource/Death_in_the_Afternoon Title: Death in the Afternoon Release Date: 1932\n",
      "Book: http://dbpedia.org/resource/By-Line:_Ernest_Hemingway Title: By-Line: Ernest Hemingway Release Date: 1967\n",
      "Book: http://dbpedia.org/resource/A_Moveable_Feast Title: A Moveable Feast Release Date: 1964\n",
      "Book: http://dbpedia.org/resource/For_Whom_the_Bell_Tolls Title: For Whom the Bell Tolls Release Date: 1940-10-21\n",
      "Book: http://dbpedia.org/resource/Islands_in_the_Stream_(novel) Title: Islands in the Stream (novel) Release Date: 1970\n",
      "Book: http://dbpedia.org/resource/The_Fifth_Column_and_the_First_Forty-Nine_Stories Title: The Fifth Column and the First Forty-Nine Stories Release Date: 1938\n",
      "Book: http://dbpedia.org/resource/Across_the_River_and_into_the_Trees Title: Across the River and into the Trees Release Date: 1950\n",
      "Book: http://dbpedia.org/resource/Green_Hills_of_Africa Title: Green Hills of Africa Release Date: 1935-10-25\n",
      "Book: http://dbpedia.org/resource/To_Have_and_Have_Not Title: To Have and Have Not Release Date: 1937\n",
      "Book: http://dbpedia.org/resource/The_Complete_Short_Stories_of_Ernest_Hemingway Title: The Complete Short Stories of Ernest Hemingway Release Date: 1987\n"
     ]
    }
   ],
   "source": [
    "#Exercice 3.1\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "SELECT ?book ?title ?releaseDate\n",
    "WHERE {\n",
    "    ?book dbp:author <http://dbpedia.org/resource/Ernest_Hemingway> .\n",
    "    ?book rdfs:label ?title .\n",
    "    ?book dbp:releaseDate ?releaseDate .\n",
    "    FILTER (lang(?title) = 'en') \n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "for result in results[\"results\"][\"bindings\"]:\n",
    "    print(f\"Book: {result['book']['value']} Title: {result['title']['value']} Release Date: {result['releaseDate']['value']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#Exercice 3.2\n",
    "\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "sparql.setQuery(\"\"\"\n",
    "    ASK {\n",
    "    <http://dbpedia.org/resource/Don’t_Look_Up_(2021_film)> dbo:director <http://dbpedia.org/resource/Adam_McKay> .\n",
    "}\n",
    "\"\"\")\n",
    "sparql.setReturnFormat(JSON)\n",
    "results = sparql.query().convert()\n",
    "\n",
    "print(results['boolean'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3_env",
   "language": "python",
   "name": "py3_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
